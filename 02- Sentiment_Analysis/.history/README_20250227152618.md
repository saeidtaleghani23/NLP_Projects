# Sentiment Analysis using Transformer Models

## Overview
This project trains and evaluates four transformer-based models—BERT, DistilBERT, RoBERTa, and ALBERT—for sentiment analysis. It supports three datasets: `IMDB`, `Yelp Polarity`, and `Amazon Polarity`. The project is designed to be configurable through a `config.yml` file and allows users to specify datasets and configurations via command-line arguments.

## Features
- Supports multiple transformer models (BERT, DistilBERT, RoBERTa, ALBERT)
- Allows sentiment analysis on `IMDB`, `Yelp Polarity`, and `Amazon Polarity` datasets
- Uses `config.yml` for model and training configuration
- Command-line arguments for dynamic dataset and configuration selection
- Evaluates models using accuracy, precision, recall, and F1-score
- Saves trained models and results for comparison

## Installation
To set up the environment, use the provided `env.yml` file:

```sh
conda env create -f env/env.yml
conda activate sentiment_env
```

## Usage
### Running the Training Script
Run the following command to train and evaluate the models:

```sh
python main.py --config ./config/config.yml --dataset imdb
```

### Command-Line Arguments
| Argument  | Description |
|-----------|-------------|
| `--config`  | Path to the configuration YAML file (default: `./config/config.yml`) |
| `--dataset` | Dataset name: `imdb`, `yelp_polarity`, or `amazon_polarity` |

## Configuration
The `config.yml` file (stored in the `config/` directory) allows customization of training parameters such as batch size, learning rate, number of epochs, and model selection. Example:

```yaml
DATASET:
  dataset: imdb

MODEL:
  bert: bert-base-uncased
  distilbert: distilbert-base-uncased
  roberta: roberta-base
  albert: albert-base-v2

TRAIN:
  batch_size: 16
  num_epochs: 3
  learning_rate: 5e-5
  output_dir: ./outputs
  logging_dir: ./logs
  model_save_dir: ./saved_models
  results_dir: ./results
```

## Evaluation Metrics
After training, the following metrics are computed for model evaluation:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-score**

## Results
Evaluation results are saved in the `results/` directory as YAML files with timestamps. A final comparison of model performance is displayed after training.

## Project Structure
```
.
├── config/                  # Configuration files
│   ├── config.yml
├── env/                     # Environment setup
│   ├── env.yml
├── results/                 # Evaluation results
├── saved_models/            # Saved model checkpoints
├── main.py                  # Main training and evaluation script
├── README.md                # Project documentation
```

## Dependencies
- Python 3.12+
- Transformers
- Datasets
- PyTorch
- Scikit-learn
- PyYAML
- NumPy

## License
This project is licensed under the MIT License.

## Author
[Your Name]

